{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 1_1_linear_regression.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "collapsed": true,
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "4e0f42c6439b082fc537fe0ec1f0d801",
          "grade": false,
          "grade_id": "cell-68b905920fbe16ce",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "lExLEg0BWdTC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 1: Linear Regression"
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "b669f6696864bf529ca9f9af6217d0e4",
          "grade": false,
          "grade_id": "cell-1c5581fc981be7ed",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "YrsY28TtWdTL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Execute this code block to install dependencies when running on colab\n",
        "try:\n",
        "    import torch\n",
        "except:\n",
        "    from os.path import exists\n",
        "    from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "    platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "    cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "    accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "    !pip install -q http://download.pytorch.org/whl/{accelerator}/torch-1.0.0-{platform}-linux_x86_64.whl torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "cb81ed04ab5982d538771590f45802ce",
          "grade": false,
          "grade_id": "cell-8128a87aa950cfd5",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "sKMbwTzAWdTY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Getting started \n",
        "\n",
        "At its heart, PyTorch is just a library for manipulating tensors. We're going to start learning how to use \n",
        "PyTorch by looking at how we can implement simple linear regression. \n",
        "\n",
        "Code speaks better than words, so lets start by looking at a bit of pytorch code to generate some 2d data to regress:"
      ]
    },
    {
      "metadata": {
        "id": "Vp3Xn96eWdTc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Generate some data points on a straight line perturbed with Gaussian noise\n",
        "N = 1000 # number of points\n",
        "theta_true = torch.Tensor([[1.5], [2.0]]) # true parameters of the line\n",
        "\n",
        "X = torch.rand(N, 2) \n",
        "X[:, 1] = 1.0\n",
        "y = X @ theta_true + 0.1 * torch.randn(N, 1) # Note that just like in numpy '@' represents matrix multiplication and A@B is equivalent to torch.mm(A, B) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "15906b16ca0952809a0d0f6821439957",
          "grade": false,
          "grade_id": "cell-67f4ae4fafb20c75",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "eau18POaWdTm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The above code generates $(x,y)$ data according to $y = 1.5x + 2$, with the $x$'s chosen from a uniform distribution. The $y$'s are additionally purturbed by adding an amount $0.1z$, where $z\\sim \\mathcal{N}(0,1)$ is a sample from a standard normal distribution. \n",
        "\n",
        "Note that we represent our $x$'s as a two-dimensional (row) vector with a 1 in the second element so that the offset can be rolled into the matrix multiplication for efficiency:\n",
        "\n",
        "\\begin{align}\n",
        "    y &= \\mathbf{X}\\begin{bmatrix}\n",
        "           1.5 \\\\\n",
        "           2\n",
        "         \\end{bmatrix}\n",
        "  \\end{align}\n",
        "\n",
        "Let's use `matplotlib` to draw a scatter so we can be sure of what our data looks like:"
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "77838c69c9e63098d1e93e6ea06ca513",
          "grade": false,
          "grade_id": "cell-36f16b5ab42ab959",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "v9ns0KADWdTr",
        "colab_type": "code",
        "outputId": "0ca351a0-bf9e-4f0c-e581-e8a3a2718ad3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        }
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.scatter(X[:,0].numpy(), y.numpy())\n",
        "plt.show()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD7CAYAAACRxdTpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCEIVIuIuCIJQhfz/l+DsUXNdCQsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "dfce8172c55747e4c8aa47d6bd3ca291",
          "grade": false,
          "grade_id": "cell-26114c36c1f243e4",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "4C0ZTjW8WdT1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "__Make sure you understand how the code above is generating data; feel free to change the parameters to see what effect they have.__\n",
        "\n",
        "Now, lets consider the situation where we have been given the tensors $X$ and $y$ and wish to compute the regression parameters. Our model looks like $\\mathbf{y} = \\mathbf{X\\theta}$, and we wish to recover the parameters $\\theta$. \n",
        "\n",
        "As the problem is both overcomplete (only two data pairs are required to find $\\theta$), and the data is noisy, we can use the Moore-Penrose Pseudoinverse to find the least-squares solution to $\\theta$: $\\theta = \\mathbf{X^+y}$. PyTorch has a built-in pseudoinverse method (`pinverse`) that can do all the work for us:"
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "3226facb17e2a669d112c2b1700a8daa",
          "grade": false,
          "grade_id": "cell-3ae7c2a27cf28ee6",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "cKz55cdtWdT3",
        "colab_type": "code",
        "outputId": "11fa17fe-d047-4d3c-ae34-6e69cce3d673",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "# direct solution using moore-penrose pseudo inverse\n",
        "X_inv = torch.pinverse(X)\n",
        "theta_pinv = torch.mm(X_inv, y)\n",
        "print(theta_pinv)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1.4988],\n",
            "        [2.0041]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "0c6a1fb676027787f94fc1b27eb815ed",
          "grade": false,
          "grade_id": "cell-2f7080b24616a7d1",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "vveuN7CmWdT-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Running the above code should give you a solution vector for $\\theta$ that is very similar to the true parameter vector (`theta_true`). "
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "c2d7e31bd39f66e7c4122c6fffc42006",
          "grade": false,
          "grade_id": "cell-ca707e4c78ca3acc",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "niHQVwisWdUB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercise: computing the pseudoinverse from the Singular Value Decomposition.\n",
        "\n",
        "The standard way of computing the pseudoinverse is by using the Singular Value Decomposition (SVD). The SVD is defined as: $\\mathbf{X} = \\mathbf{U}\\Sigma\\mathbf{V}^\\top$. The pseudoinverse is thus $\\mathbf{X}^+ = \\mathbf{V}\\Sigma^{-1}\\mathbf{U}^\\top$ where $\\Sigma^{-1}$ is a diagonal matrix in which the reciprocal of the corresponding non-zero elements in $\\Sigma$ has been taken.\n",
        "\n",
        "__Use the code block below to compute the parameter vector using the SVD directly rather than the through the `pinverse` method.__ You need to store your manually computed pseudoinverse in `X_inv_svd`. Useful methods will be `torch.svd()` to compute the SVD, `[Tensor].t()` to transpose a matrix and `torch.diag()` to form a diagonal matrix from a vector."
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "96170c2a936223004245b58ae10a2b62",
          "grade": false,
          "grade_id": "cell-4d422a83842ebab7",
          "locked": false,
          "schema_version": 1,
          "solution": true
        },
        "id": "E8M6qIRuWdUF",
        "colab_type": "code",
        "outputId": "b4ffd1d8-3cda-4b6f-aec2-b6c2874b2eb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "u,s,v = torch.svd(X)\n",
        "\n",
        "X_inv_svd = v.t()@torch.inverse(s.diag())@u.t()\n",
        "\n",
        "theta_pinv_svd = torch.mm(X_inv_svd, y)\n",
        "print(theta_pinv_svd)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1.4988],\n",
            "        [2.0041]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "d9743a1c36a687bf2adb629500247a05",
          "grade": true,
          "grade_id": "cell-4e7e9488b81a58b8",
          "locked": true,
          "points": 2,
          "schema_version": 1,
          "solution": false
        },
        "id": "G5PXdxIrWdUS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "assert(torch.all(torch.lt(torch.abs(torch.add(theta_pinv, -theta_pinv_svd)), 1e-6)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iBIJRFlBWdUb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Gradient based Linear Regression\n",
        "\n",
        "Fundamentally, with linear regression we are trying to find a solution vector, $theta$ that minimises $f(\\theta) = 0.5\\|\\mathbf{X}\\theta - \\mathbf{y}\\|_2^2$. \n",
        "\n",
        "We've already seen how this can be minimised directly using the pseudoinverse, but it could also be minimised by using gradient descent: $\\theta \\gets \\theta - \\alpha f'(\\theta)$. (_Interesting aside_: SVD (and thus the pseudoinverse) can also be solved using gradient methods - in fact this becomes the only practical way for really large matrices.).\n",
        "\n",
        "__Use the following block to derive and write down the gradient, $f'(\\theta)$, of $f(\\theta)$__. Note that you can insert latex code by wrapping expressions in dollar symbols."
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "0197441552c3f53e1fb98b1ee6d232b8",
          "grade": true,
          "grade_id": "cell-210cc7d9ab3905e5",
          "locked": false,
          "points": 2,
          "schema_version": 1,
          "solution": true
        },
        "id": "iSvAH7F2WdUd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "$f(\\theta)=0.5\\|\\mathbf{X}\\theta - \\mathbf{y}\\|_2^2 $ \n",
        "\n",
        "$f(\\theta)=0.5(\\sqrt{(\\mathbf{X}\\theta-\\mathbf{y})^2}^2) $\n",
        "\n",
        "$f(\\theta)=0.5\\sum_i^n(x_i\\theta-y_i)^2$\n",
        "\n",
        "$\\text{From matrix calculus, we look at vector-by-vector derivatives:}$\n",
        "\n",
        "$\\frac{\\partial(x_1\\theta - y_1)^2}{\\partial(\\theta)} = \\begin{bmatrix} \\frac{\\partial(x_i\\theta-y_i)^2}{\\partial(\\theta)} & \\frac{\\partial(x_i\\theta-y_i)^2}{\\partial(\\theta)} \\\\ \\frac{\\partial(x_i\\theta-y_i)^2}{\\partial(\\theta)} & \\frac{\\partial(x_i\\theta-y_i)^2}{\\partial(\\theta)}\n",
        "\\end{bmatrix} = \\begin{bmatrix} x_1 & x_2 \\\\ x_1 & x_2 \\end{bmatrix}$\n",
        "\n",
        "$f'(\\theta)=0.5*2\\sum_i^n(x_i\\theta - y_i)x_i $\n",
        "\n",
        "$f'(\\theta)=\\sum_i^n(x_i\\theta - y_i)x_i$"
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "4d1c00f47e8080e27a49ba3999c0b342",
          "grade": false,
          "grade_id": "cell-d8813ea915bc08f7",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "9iBYZwkpWdUh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "__Now complete the following code block to implement your gradient as pytorch code:__"
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "65ea0d0a4244f58b618bd1eb0272fa3e",
          "grade": false,
          "grade_id": "cell-640dcd4113de31d6",
          "locked": false,
          "schema_version": 1,
          "solution": true
        },
        "id": "-OKGb6gnWdUj",
        "colab_type": "code",
        "outputId": "4d2d9d90-bbd3-4449-e42e-983e96b09b1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "def linear_regression_loss_grad(theta, X, y):\n",
        "    grad = torch.zeros_like(X[0])\n",
        "    \n",
        "    for i in range(len(X)):\n",
        "      grad += (X[i]@theta - y[i])*X[i]\n",
        "\n",
        "    return grad.reshape(-1,1)\n",
        "  \n",
        "print(linear_regression_loss_grad(theta_true, X, y))\n",
        "  "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-1.6615],\n",
            "        [-3.4747]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "1ff96ad5def7b717aea28ab5ccaedc04",
          "grade": true,
          "grade_id": "cell-9cdc211cfd5cab66",
          "locked": true,
          "points": 2,
          "schema_version": 1,
          "solution": false
        },
        "id": "KtLA5e1-WdUv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "assert(linear_regression_loss_grad(torch.zeros(2,1), X, y).shape == (2,1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "646a7eb8b434567047a8278198a74ffa",
          "grade": false,
          "grade_id": "cell-298e11818c4e9b65",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "sE8CgcSFWdU1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we can plug that gradient function into a basic gradient descent solver and check that the solution is close to what we get with the pseudoinverse:"
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "0ee711fe83b16cd14129aec603c141e9",
          "grade": false,
          "grade_id": "cell-cef34abd27fe76a8",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "6whUSu7aWdU5",
        "colab_type": "code",
        "outputId": "2cce8bae-4fc9-4375-8ad5-5c446f7d2252",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "cell_type": "code",
      "source": [
        "alpha = 0.001\n",
        "theta = torch.Tensor([[0], [0]])\n",
        "for e in range(0, 200):\n",
        "    gr = linear_regression_loss_grad(theta, X, y)\n",
        "    theta -= alpha * gr\n",
        "\n",
        "print(theta)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-f338d3b44f88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_regression_loss_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtheta\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-26c4e69a4b27>\u001b[0m in \u001b[0;36mlinear_regression_loss_grad\u001b[0;34m(theta, X, y)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m       \u001b[0mgrad\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mtheta\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "8a785d76923ccbb7fa7b953a67377385",
          "grade": false,
          "grade_id": "cell-47be64696ed6859b",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "A8LQLexnWdU_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Real data\n",
        "\n",
        "Doing linear regression on synthetic data is a great way to understand how PyTorch works, but it isn't quite as satisfying as working with a real dataset. Let's now apply or understanding of computing linear regression parameters to a dataset of house prices in Boston.\n",
        "\n",
        "We'll load the dataset using scikit-learn and perform some manipulations in the following code block:"
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "68160322d431987aaa1330088bd7efa5",
          "grade": false,
          "grade_id": "cell-3d4dfae2229c46df",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "4CeNFiZFWdVE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "c5fb4a2c-1a0d-44ee-c1bd-b75acafb65a1"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_boston\n",
        "\n",
        "X, y = tuple(torch.Tensor(z) for z in load_boston(True)) #convert to pytorch Tensors\n",
        "X = X[:, [2,5]] # We're just going to use features 2 and 5, rather than using all of of them\n",
        "X = torch.cat((X, torch.ones((X.shape[0], 1))), 1) # append a column of 1's to the X's\n",
        "y = y.reshape(-1, 1) # reshape y into a column vector\n",
        "print('X:', X.shape)\n",
        "print('y:', y.shape)\n",
        "\n",
        "# We're also going to break the data into a training set for computing the regression parameters\n",
        "# and a test set to evaluate the predictive ability of those parameters\n",
        "perm = torch.randperm(y.shape[0])\n",
        "X_train = X[perm[0:253], :]\n",
        "y_train = y[perm[0:253]]\n",
        "X_test = X[perm[253:], :]\n",
        "y_test = y[perm[253:]]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X: torch.Size([506, 3])\n",
            "y: torch.Size([506, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "W2razThKWdVQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "__Use the following code block to compute the regression parameters using the training data in the variable `theta` by solving using the pseudoinverse directly:__"
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "4f967bc6cfdf19e504fbc602fde368a7",
          "grade": false,
          "grade_id": "cell-b7854194044222ed",
          "locked": false,
          "schema_version": 1,
          "solution": true
        },
        "id": "aev5hSftWdVS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "1e2cfe70-d795-4e9f-9aeb-51253bc4ad5d"
      },
      "cell_type": "code",
      "source": [
        "# compute the regression parameters in variable theta\n",
        "X_inv = torch.pinverse(X)\n",
        "theta = X_inv@y\n",
        "print(theta)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ -0.3347],\n",
            "        [  7.8221],\n",
            "        [-22.8983]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EAhetWKIWdVY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can now print out the error achieved on the test set, as well as the parameter vector:"
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "724495cb56038fbab28377c134c95d92",
          "grade": true,
          "grade_id": "cell-a97d86a0f20894f5",
          "locked": true,
          "points": 1,
          "schema_version": 1,
          "solution": false
        },
        "id": "ykRhMs6gWdVZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "256875ae-5a72-464e-d906-3140a74c48b2"
      },
      "cell_type": "code",
      "source": [
        "assert(theta.shape == (3,1))\n",
        "\n",
        "print(\"Theta: \", theta.t())\n",
        "print(\"MSE of test data: \", torch.nn.functional.mse_loss(X_test @ theta, y_test))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Theta:  tensor([[ -0.3347,   7.8221, -22.8983]])\n",
            "MSE of test data:  tensor(45.9774)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "d794e7faf7455d8154d6fcab85e15e4d",
          "grade": false,
          "grade_id": "cell-f1a7b0a932a8ba2e",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "MhDmQE56WdVn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's try using gradient descent:"
      ]
    },
    {
      "metadata": {
        "id": "z_1E0kMiWdVr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "alpha = 0.00001\n",
        "theta_gd = torch.rand((X_train.shape[1], 1))\n",
        "for e in range(0, 10000):\n",
        "    gr = linear_regression_loss_grad(theta_gd, X_train, y_train)\n",
        "    theta_gd -= alpha * gr\n",
        "\n",
        "print(\"Gradient Descent Theta: \", theta_gd.t())\n",
        "print(\"MSE of test data: \", torch.nn.functional.mse_loss(X_test @ theta_gd, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "f2b9cd4ed786c8a74d31e22d7f7cd7cc",
          "grade": false,
          "grade_id": "cell-7afbfb1f2aaaa9a4",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "cNoiIcQCWdVy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "__Use the following block to note down any observations you can make about the choice of learning rate and number of iterations in the above code. What factors do you think influence the choice?__"
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "487706e65407095404aaaf95ab1dc586",
          "grade": true,
          "grade_id": "cell-1e987d4019c368ba",
          "locked": false,
          "points": 3,
          "schema_version": 1,
          "solution": true
        },
        "id": "kJsJ6Tm2WdV7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "de8db286-4086-472e-a793-7910555c3cf2"
      },
      "cell_type": "code",
      "source": [
        "\"\"\" \n",
        "Increasing the number of interations and decreasing the learning rate produces us results as close to or even better than previous result\n",
        "The number of iterations is dependant on the time one has to run the gradient descent or on the actual computational power of the machine.\n",
        "The more iterations, the better the results we get in the end, but the more time it takes for the gradient descent to find a solution.\n",
        "The learning rate has to be adjusted based on the performance of the algorithm. \n",
        "A higher learning rate might result in a faster result, but there is a chance the gradient descent might overshoot and thus not finding an optimal solution.\n",
        "A small learning rate might make the gradient descent find a solution rather slow, sometimes not even hitting the optimal solution.\n",
        "\"\"\"\n",
        "alpha = 0.000007\n",
        "theta_gd = torch.rand((X_train.shape[1], 1))\n",
        "for e in range(0, 200000):\n",
        "    gr = linear_regression_loss_grad(theta_gd, X_train, y_train)\n",
        "    theta_gd -= alpha * gr\n",
        "\n",
        "print(\"Gradient Descent Theta: \", theta_gd.t())\n",
        "print(\"MSE of test data: \", torch.nn.functional.mse_loss(X_test @ theta_gd, y_test))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-25ef267a9642>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtheta_gd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_regression_loss_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta_gd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mtheta_gd\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-26c4e69a4b27>\u001b[0m in \u001b[0;36mlinear_regression_loss_grad\u001b[0;34m(theta, X, y)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m       \u001b[0mgrad\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mtheta\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "138abf4c0e59e16e5be8c847dff30f8d",
          "grade": false,
          "grade_id": "cell-809a19fe6970a99b",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "7Uo_cwGaWdWD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finally, just so we can visualise what our model has learned, we can plot the predicted house prices (from both the direct solution and from gradient descent) along with the true value for each of the houses in the test set (ordered by increasing true value):"
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "f486683e0b3e4d14da067504831f6cd3",
          "grade": false,
          "grade_id": "cell-64d6a9203da564f1",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "vkbwJWotWdWG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "perm = torch.argsort(y_test, dim=0)\n",
        "plt.plot(y_test[perm[:,0]].numpy(), '.', label='True Prices')\n",
        "plt.plot((X_test[perm[:,0]] @ theta).numpy(), '.', label='Predicted (pinv)')\n",
        "plt.plot((X_test[perm[:,0]] @ theta_gd).numpy(), '.', label='Predicted (G.D.)')\n",
        "plt.xlabel('House Number')\n",
        "plt.ylabel('House Price ($,000s)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}